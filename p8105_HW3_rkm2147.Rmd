---
title: "Homework 3"
subtitle: "Part 3"
output: github_document
author: Ronae McLin rkm2147
---


```{r}
library(tidyverse)
library(patchwork)
library(ggridges)


knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)
theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_color_viridis_d

scale_fill_discrete = scale_fill_viridis_d
```

# Problem 1

```{r}
library(p8105.datasets)
data("instacart")
```


Instacart is an online service that allows for users to shop locally from various stores that can then be delivered within NYC. There are a total of 1,384,617 observations contained within this dataset. This dataset contains `r nrow(instacart)` rows and `r ncol(instacart)` columns. Observations are the level of items in orders by user. Variables include: `r names(instacart)`. 

**There are 134 aisles, and aisles in which the top 3 most ordered items are ordered from are fresh vegetables, fresh fruits, and packaged vegetables & fruits** 

```{r}
instacart %>% 
  count(aisle) %>% 
  arrange(desc(n))
```


**Plot of items ordered from each aisle, more than 10000.**

```{r}
instacart %>% 
  count(aisle) %>% 
  filter(n > 10000) %>% 
  ggplot(aes( x = aisle, y = n)) + geom_point() + theme(axis.text.x = element_text(angle = 270, vjust = 0.5, hjust = 1)) + labs(
    title = "items ordered from each aisle",
    x = "aisle",
    y = "number of items"
  )
```

**A table showing the three most popular items in each of the aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”.**

```{r}
instacart %>% 
	filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>% 
	group_by(aisle) %>% 
	count(product_name) %>% 
	mutate(rank = min_rank(desc(n))) %>% 
	filter(rank < 4) %>% 
	arrange(aisle, rank) %>% 
	knitr::kable()
```

**A table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week**

```{r}
instacart %>% 
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>% 
  group_by(product_name, order_dow) %>% 
  summarize(mean_hour = mean(order_hour_of_day)) %>% 
  pivot_wider(
    names_from = order_dow,
    values_from = mean_hour
  ) %>% 
  knitr::kable()
```

# Problem 2

**Load data & tidy the data appropriately** 
```{r}
accel_df = read_csv("./accel_data.csv") %>% 
  janitor::clean_names() %>% 
  mutate(
    day = as.factor(day)
  ) %>% 
  pivot_longer(
    activity_1:activity_1440,
    names_to = "minute",
    names_prefix = "activity_",
      values_to = "value"
  ) %>% 
  mutate(
    minute = as.numeric(minute)
  ) %>% mutate(
  weekday = if_else(day %in% c("Saturday", "Sunday"), "FALSE", "TRUE")) %>% 
  mutate(weekday = as.factor(weekday)
         ) %>%
  mutate(
    day =  
  forcats::fct_relevel(day, c("Monday", "Tuesday", "Wednesday", "Thursday","Friday","Saturday", "Sunday"))
  )
  
 
```


**The accelerometer data describes information collected on a 63 year-old male with congesitve heart failure. This data includes the variables: `r names(accel_df)`. The weekday variable describes if observations occurred during the weekday (Mon-Fri) or during the weekend (Sat-Sun). There are a total of `r nrow(accel_df)` rows of observations for this frame. Observations were collected for a total of 5 weeks. There are 1440 minutes in a 24 hour period, so the amount of observations seems appropriate**
 
*Create activity table based on weeks*
```{r}
accel_df %>% 
  group_by(day, week) %>% 
  summarize(sum_value = sum(value)) %>%
  pivot_wider(
    names_from = day,
    values_from = sum_value
  ) %>% 
  knitr::kable()

```
 **From the created table, we can see a trend that the sum of activity is the lowest on saturdays, notably during the fourth and fifth week.**


```{r}

accel_df %>% 
ggplot(aes(x = minute, y = value, color = day, group = day_id)) + geom_line(alpha = .4) + geom_smooth(aes(group = day)) + labs(
    title = "Distribution of activitiy levels over a 24 period for the course of a week",
    x = "minutes",
    y = "activity values"
  ) + theme(plot.title = element_text(size = 10))
```

**From the observed plot, we can see that at the start of the 24 hour period, activity is considerably low.  We can assume that this is during a period of rest/sleep.  As the day progresses, activity level increases appropriately, with a surge towards the end of the recording period.**

# Problem 3


```{r}
library(p8105.datasets)
data("ny_noaa") 
```

**The NY NOAA is a large data set containing information for all New York state weather stations from January 1, 1981 through December 31, 2010. Each weather station may collect only a subset of these variables, and therefore the resulting dataset contains extensive missing data. Variables include: `r names(ny_noaa)`. There are a total of `r nrow(ny_noaa)` rows.** 

*Load data, split the date variable, and make appropriate class changes*

```{r}
weather_df = ny_noaa %>% 
separate(date, into = c("year", "month", "day"))  %>% 
  mutate(tmax = as.numeric(tmax)) %>% 
  mutate(tmin = as.numeric(tmin)) %>% 
  mutate(year = as.factor(year)) %>% 
  mutate(month = as.factor(month)) %>%
  mutate(id = as.factor(id)) %>% 
  mutate(
    prcp = prcp/10,
    tmax = tmax/10,
    tmin = tmin/10
  ) 
```


*Snow fall code chunk*
```{r}
weather_df %>% 
  count(month, year,snow) %>% 
  group_by(month, year) 
```

**For snowfall, the most commonly observed value is 0, this makes sense since it doesn't snow all year round in most places due to the seasonality of climate.**

*Creating a two panel plot*
```{r}
weather_df %>% 
  filter(month %in% c("01","07")) %>% 
  group_by(id, month, year) %>% 
  summarize(mean_tmax = mean(tmax, na.rm = TRUE)) %>% 
  ggplot(aes(x = year, y = mean_tmax, color = month)) + geom_path() + geom_point() +
  labs(
    title = "Average max temperature across the years January vs July",
    x = "year",
    y = "average max temperatures"
  ) +
  facet_grid(~month) + theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
```

**Observations across the years appear to be constant for both the months of January and July. However for the month of January, there is a noticeable increase in the average max temperature between the period of 1980 to 1990. As expected, the average max temperature is greater for the month of July compared to January. There are a few outliers of significance I notice on the plot, but I don't interpret them to be extreme in nature.  The most noticeable trend being that there is more spread of the data points within January.**


```{r}
temp_plot = 
weather_df %>% 
  select(year, tmax, tmin) %>% 
  drop_na(tmax,tmin) %>% 
  ggplot(aes(x = tmax, y = tmin)) + geom_hex() + labs(
    title = "The max and min temperatures across the NY NOAA dataset",
    x = "Maximum temperature",
    y = "Minimum temperature"
  ) + theme(legend.position = "right", plot.title = element_text(size = 5))
```


```{r}
snow_plot = 
  weather_df %>% 
  select(snow, year) %>% 
  drop_na(snow) %>% 
  filter(snow > 0 & snow < 100) %>% 
  ggplot(aes(x = year, y = snow)) + geom_violin() + labs(
    title = "the distribution of snowfall values greater than 0 and less than 100 by year",
    x = "year",
    y = "snowfall in mm"
  ) + theme(axis.text.x = element_text(angle = 270, vjust = 0.5, hjust = 1), plot.title = element_text(size = 5))
```

```{r}
snow_plot + temp_plot
```

**Snow fall across the years is appropriate considering the seasonality associated with NY state. We also see that as the maximum temperature increases, the minimum increases as well.  This is displaying a positive correlation between the two variables.**
