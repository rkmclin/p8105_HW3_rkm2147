---
title: "Homework 3"
subtitle: "Part 3"
output: github_document
author: Ronae McLin rkm2147
---


```{r}
library(tidyverse)
library(patchwork)

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)
theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_color_viridis_d

scale_fill_discrete = scale_fill_viridis_d
```

# Problem 1

```{r}
library(p8105.datasets)
data("instacart")
```


Instacart is an online service that allows for users to shop locally from various stores that can then be delivered within NYC. There are a total of 1,384,617 observations contained within this dataset. This dataset contains `r nrow(instacart)` rows and `r ncol(instacart)` columns. Observations are the level of items in orders by user. Variables include: `r names(instacart)`. 

**There are 134 aisles, and aisles in which the top 3 most ordered items are ordered from are fresh vegetables, fresh fruits, and packaged vegetables & fruits** 

```{r}
instacart %>% 
  count(aisle) %>% 
  arrange(desc(n))
```


**Plot of items ordered from each aisle, more than 10000.**

```{r}
instacart %>% 
  count(aisle) %>% 
  filter(n > 10000) %>% 
  ggplot(aes( x = aisle, y = n)) + geom_point() + theme(axis.text.x = element_text(angle = 270, vjust = 0.5, hjust = 1))
```

**A table showing the three most popular items in each of the aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”.**

```{r}
instacart %>% 
	filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>% 
	group_by(aisle) %>% 
	count(product_name) %>% 
	mutate(rank = min_rank(desc(n))) %>% 
	filter(rank < 4) %>% 
	arrange(aisle, rank) %>% 
	knitr::kable()
```

**A table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week**

```{r}
instacart %>% 
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>% 
  group_by(product_name, order_dow) %>% 
  summarize(mean_hour = mean(order_hour_of_day)) %>% 
  pivot_wider(
    names_from = order_dow,
    values_from = mean_hour
  ) %>% 
  knitr::kable()
```

# Problem 2

**Load data & Tidy the data appropriately **
```{r}
accel_df = read_csv("./accel_data.csv") %>% 
  janitor::clean_names() %>% 
  mutate(
    day = as.factor(day)
  ) %>% 
  pivot_longer(
    activity_1:activity_1440,
    names_to = "minute",
    names_prefix = "activity_",
      values_to = "value"
  ) %>% 
  mutate(
    minute = as.numeric(minute)
  ) %>% mutate(
  weekday = if_else(day %in% c("Saturday", "Sunday"), "FALSE", "TRUE")) %>% 
  mutate(weekday = as.factor(weekday)
         ) %>%
  mutate(
    day =  
  forcats::fct_relevel(day, c("Monday", "Tuesday", "Wednesday", "Thursday","Friday","Saturday", "Sunday"))
  )
  
 
```


**The accelerometer data describes information collected on a 63 year-old male with congesitve heart failure. This data includes the variables: `r names(accel_df)`. The weekday variable describes if observations occurred during the weekday (Mon-Fri) or during the weekend (Sat-Sun). There are a total of `r nrow(accel_df)` observations for this frame. Observations were collected for a total of 5 weeks. There are 1440 minutes in a 24 hour period, so the amount of observations seems appropriate**
 

```{r}
accel_df %>% 
  group_by(day, week) %>% 
  summarize(sum_value = sum(value)) %>%
  pivot_wider(
    names_from = day,
    values_from = sum_value
  ) %>% 
  knitr::kable()

```
 **From the created table, we can see a trend that the sum of activity is the lowest on saturdays, notably during the fourth and fifth week.**

```{r}

accel_df %>% 
ggplot(aes(x = minute, y = value, color = day, group = day_id)) + geom_line(alpha = .4) + geom_smooth(aes(group = day))
```

**From the observed plot, we can see that at the start of the 24 hour period, activity is considerably low.  We can assume that this is during a period of rest/sleep.  As the day progresses, activity level increases appropriately, with a surge towards the end of the recording period.**

# Problem 3


```{r}
library(p8105.datasets)
data("ny_noaa") 
```

**The NY NOAA is a large data set containing information for all New York state weather stations from January 1, 1981 through December 31, 2010. Each weather station may collect only a subset of these variables, and therefore the resulting dataset contains extensive missing data. Variables include `r names(ny_noaa)`. There are a total of `r nrow(ny_noaa)` rows.** 


```{r}
ny_noaa = ny_noaa %>% 
separate(date, into = c("year", "month", "day"), convert = TRUE)
```


```{r}
ny_noaa %>% 
  mutate(
   tmax = as.numeric(tmax),
    tmin = as.numeric(tmin)
  ) %>% 
  mutate(
    prcp = prcp / 10,
    tmax = tmax / 10,
    tmin = tmin / 10
  ) %>% 
  	group_by(month) %>% 
	count(snow) %>% 
	mutate(rank = min_rank(desc(snow))) %>% 
  filter(rank < 5) %>%
  knitr::kable()
```


Make a two-panel plot showing the average max temperature in January and in July in each station across years. Is there any observable / interpretable structure? Any outliers?

```{r}
ny_noaa %>% 
  mutate(
   tmax = as.numeric(tmax),
    tmin = as.numeric(tmin)
  )%>% 
  select(month, tmax, year) %>% 
  group_by(month, year) %>% 
  filter(month %in% c(1,7)) %>% 
  drop_na() %>% 
  summarize(mean_tmax = mean(tmax)) %>% 
  ggplot(aes(x = year, y = mean_tmax)) + geom_point() + geom_smooth() +
  labs(
    title = "jan and july average",
    x = "year information",
    y = "data i need"
  ) +
  facet_grid(~month, scales = "free_y")
```


Make a two-panel plot showing (i) tmax vs tmin for the full dataset (note that a scatterplot may not be the best option); and (ii) make a plot showing the distribution of snowfall values greater than 0 and less than 100 separately by year.

```{r}
ny_noaa %>% 
  select(year, tmax, tmin) %>% 
  drop_na(tmax,tmin) %>% 
  ggplot(aes(x = tmax, y = tmin)) + geom_bin2d()
```

